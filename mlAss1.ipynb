{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f714fc3f-400b-4ef1-baa7-e17430262130",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of information for certain variables or observations. These missing values can arise due to various reasons such as data entry errors, equipment failures, or intentional omission. It is essential to handle missing values in a dataset because they can lead to biased analysis and inaccurate conclusions if not dealt with properly.\n",
    "\n",
    "Handling missing values is important for several reasons:\n",
    "\n",
    "Preventing Biased Results: Analyzing data with missing values can lead to biased estimates and inaccurate predictions.\n",
    "\n",
    "Maintaining Data Integrity: Missing values can affect the integrity of the dataset, making it unreliable for analysis.\n",
    "\n",
    "Avoiding Misinterpretation: Missing values can skew statistical analyses, leading to misinterpretation of results.\n",
    "\n",
    "Improving Model Performance: Many machine learning algorithms cannot handle missing values, so addressing them appropriately can improve the performance of predictive models.\n",
    "\n",
    "Enhancing Data Quality: By handling missing values effectively, the overall quality and reliability of the dataset are improved.\n",
    "\n",
    "Some algorithms that are not affected by missing values include:\n",
    "\n",
    "Decision Trees: Decision trees are robust to missing values as they can work directly with them during the training process.\n",
    "\n",
    "Random Forests: Random Forests are an ensemble learning method based on decision trees and can handle missing values effectively by averaging over multiple trees.\n",
    "\n",
    "k-Nearest Neighbors (k-NN): k-NN algorithm doesn't explicitly train a model, but rather memorizes the training dataset. It calculates distances between points, so missing values don't affect its performance directly.\n",
    "\n",
    "Naive Bayes: Naive Bayes algorithm calculates probabilities based on existing data and doesn't explicitly model missing values, thus it's not affected by them.\n",
    "\n",
    "Association Rule Learning Algorithms: Algorithms like Apriori for association rule learning are not directly affected by missing values as they work based on the presence or absence of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50843662-68a8-4619-842d-dadcf4d2a407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38438620-366f-4ba6-9141-e9101611da47",
   "metadata": {},
   "source": [
    "Removing rows or columns with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c0705c3-253b-4a94-8d83-58f27a5415ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after removing rows with missing values:\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n",
      "\n",
      "DataFrame after removing columns with missing values:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4],\n",
    "        'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_cleaned_rows = df.dropna(axis=0)\n",
    "\n",
    "# Remove columns with missing values\n",
    "df_cleaned_columns = df.dropna(axis=1)\n",
    "\n",
    "print(\"DataFrame after removing rows with missing values:\")\n",
    "print(df_cleaned_rows)\n",
    "\n",
    "print(\"\\nDataFrame after removing columns with missing values:\")\n",
    "print(df_cleaned_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26008bd7-3ad2-4a64-aa9d-7be4c962cc83",
   "metadata": {},
   "source": [
    "Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5f9340-fe0a-409d-9c2b-5523dcf6c878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after imputation with mean:\n",
      "          A         B\n",
      "0  1.000000  5.000000\n",
      "1  2.000000  6.666667\n",
      "2  2.333333  7.000000\n",
      "3  4.000000  8.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4],\n",
    "        'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"DataFrame after imputation with mean:\")\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b72b6-9c2d-4f69-a8ec-d9c56599a440",
   "metadata": {},
   "source": [
    "Using Predictive Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cdcadcd-e91b-4f0f-a669-c362f86d9425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after imputation using KNN:\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.5\n",
      "2  2.5  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4],\n",
    "        'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values using KNN imputer\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed_knn = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"DataFrame after imputation using KNN:\")\n",
    "print(df_imputed_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c007d2-8f26-49d0-8ef0-5f821120cc4f",
   "metadata": {},
   "source": [
    "Using Business Logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44bf96c-ee69-4a4f-b505-9a1b16a88d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after replacing missing values based on business logic:\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  0.0\n",
      "2  NaN  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4],\n",
    "        'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Replace missing values in column 'B' with a specific value based on business logic\n",
    "df['B'] = df['B'].fillna(0)  # For example, replacing missing values with 0\n",
    "\n",
    "print(\"DataFrame after replacing missing values based on business logic:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758f2d7-b366-435d-a802-1f4bce65363c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40b79a89-6b10-4666-a550-8f0aefba0f36",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation in which the distribution of classes in a dataset is skewed, meaning that one class (the minority class) is significantly underrepresented compared to another class or classes (the majority class or classes). Imbalanced data is a common issue in classification tasks, where one class may be rare or occur infrequently compared to others.\n",
    "\n",
    "Here's an example to illustrate imbalanced data: Consider a medical diagnosis task where the goal is to predict whether a patient has a rare disease (e.g., a disease that occurs in only 1% of the population). If the dataset contains 99% of instances labeled as \"healthy\" and only 1% labeled as \"diseased,\" it is an example of imbalanced data.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to several problems:\n",
    "\n",
    "Biased Models: Machine learning models trained on imbalanced data tend to be biased towards the majority class. As a result, the model may have poor performance in predicting the minority class.\n",
    "\n",
    "Poor Generalization: Imbalanced data can lead to models that generalize poorly to new, unseen data, especially for the minority class. The model may struggle to correctly classify instances belonging to the minority class in real-world scenarios.\n",
    "\n",
    "Misleading Evaluation Metrics: Traditional evaluation metrics such as accuracy can be misleading when dealing with imbalanced data. A model that always predicts the majority class can achieve high accuracy but may fail to identify instances of the minority class.\n",
    "\n",
    "Loss of Important Information: Ignoring the minority class in imbalanced data can lead to the loss of valuable information and insights present in those instances.\n",
    "\n",
    "Model Overfitting: In some cases, models trained on imbalanced data may overfit to the majority class, capturing noise rather than meaningful patterns in the data.\n",
    "\n",
    "To mitigate the problems associated with imbalanced data, various techniques can be employed, including:\n",
    "\n",
    "Resampling Methods: These methods involve either oversampling the minority class, undersampling the majority class, or a combination of both to balance the class distribution.\n",
    "\n",
    "Algorithmic Approaches: Some machine learning algorithms have built-in mechanisms to handle imbalanced data, such as class weights or specialized algorithms designed for imbalanced datasets.\n",
    "\n",
    "Ensemble Methods: Ensemble methods like bagging and boosting can be effective in improving the performance of models on imbalanced data by combining multiple weak classifiers.\n",
    "\n",
    "Synthetic Data Generation: Techniques such as SMOTE (Synthetic Minority Over-sampling Technique) can be used to generate synthetic samples for the minority class to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0ca01-9b00-4db1-ac22-0d08b1b138b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23966890-24bf-4c8f-bb08-1f9d316d3600",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are techniques used to address imbalanced data by adjusting the class distribution in the dataset.\n",
    "\n",
    "Up-sampling (Over-sampling): Up-sampling involves increasing the number of instances in the minority class to match the number of instances in the majority class. This is typically done by randomly duplicating instances from the minority class or by generating synthetic instances based on the existing minority class instances.\n",
    "\n",
    "Down-sampling (Under-sampling): Down-sampling involves reducing the number of instances in the majority class to match the number of instances in the minority class. This is typically done by randomly removing instances from the majority class until the class distribution is balanced.\n",
    "\n",
    "Here's an example illustrating when up-sampling and down-sampling may be required:\n",
    "\n",
    "Let's consider a binary classification problem where we aim to predict whether a transaction is fraudulent or not based on transaction data. Suppose we have a dataset with 95% non-fraudulent transactions (majority class) and only 5% fraudulent transactions (minority class). This dataset is highly imbalanced.\n",
    "\n",
    "When to Use Up-sampling:\n",
    "Up-sampling may be required when the minority class (fraudulent transactions) contains valuable information, and we want to avoid losing that information. In this example, up-sampling would involve increasing the number of fraudulent transactions by duplicating existing instances or generating synthetic data points. This would help balance the class distribution and ensure that the model is trained on sufficient instances of the minority class to learn meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad4b8a-154d-4ad4-aff3-0c09ee0def78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of up-sampling using Python and scikit-learn\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Upsample minority class\n",
    "fraudulent_transactions = df[df['Class'] == 1]\n",
    "non_fraudulent_transactions = df[df['Class'] == 0]\n",
    "\n",
    "# Upsample minority class to match the number of majority class\n",
    "fraudulent_upsampled = resample(fraudulent_transactions, replace=True, n_samples=len(non_fraudulent_transactions), random_state=42)\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "upsampled_df = pd.concat([non_fraudulent_transactions, fraudulent_upsampled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95872d0e-7d5f-426d-8450-35f1ab1ec51e",
   "metadata": {},
   "source": [
    "When to Use Down-sampling:\n",
    "Down-sampling may be required when the majority class contains a large number of instances that are not representative of the underlying population, leading to potential overfitting. In this example, down-sampling would involve reducing the number of non-fraudulent transactions to match the number of fraudulent transactions. This would help prevent the model from being biased towards the majority class and improve its ability to generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67a307-ff02-4c86-b71c-d356e85c32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of down-sampling using Python and scikit-learn\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Downsample majority class\n",
    "non_fraudulent_downsampled = resample(non_fraudulent_transactions, replace=False, n_samples=len(fraudulent_transactions), random_state=42)\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "downsampled_df = pd.concat([fraudulent_transactions, non_fraudulent_downsampled])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0fe85-a340-47ab-ba20-b73f22b76c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd5e7404-6857-419f-b7b3-e1572b3fafc8",
   "metadata": {},
   "source": [
    "\n",
    "Data augmentation is a technique commonly used in machine learning and computer vision to artificially increase the size of a dataset by creating new, synthetic data points from the existing data. The idea behind data augmentation is to introduce variations to the original data while preserving the label or target variable, thereby providing the model with more diverse examples to learn from and improving its generalization ability.\n",
    "\n",
    "One popular method of data augmentation, especially in the context of handling imbalanced datasets, is Synthetic Minority Over-sampling Technique (SMOTE). SMOTE is specifically designed to address the class imbalance problem by generating synthetic samples for the minority class.\n",
    "\n",
    "Here's how SMOTE works:\n",
    "\n",
    "Identify Minority Class Instances: SMOTE begins by identifying instances belonging to the minority class in the dataset.\n",
    "\n",
    "Select a Minority Instance: For each minority instance in the dataset, SMOTE selects one or more of its nearest neighbors from the same class. The number of neighbors to select is specified by a parameter called the \"k\" value.\n",
    "\n",
    "Generate Synthetic Instances: SMOTE then creates synthetic instances along the line segments connecting the minority instance to its selected neighbors. The synthetic instances are generated by interpolating the feature values of the minority instance and its neighbors.\n",
    "\n",
    "Combine Original and Synthetic Instances: Finally, the synthetic instances generated by SMOTE are added to the original dataset, effectively balancing the class distribution.\n",
    "\n",
    "By generating synthetic instances, SMOTE helps address the class imbalance problem without simply duplicating existing minority class instances, thus reducing the risk of overfitting. It also helps improve the decision boundary between classes by introducing additional data points in regions of feature space where the minority class is underrepresented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f18e68-3665-42b7-a360-c28117a96cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
